## Compiling ELL Models

ELL provides a very cool neural net model compiler that can target a specific platform and produce optimized
code for that platform.  This helps to make your models run faster on low end hardware.

So we will take the models we loaded, either from [Darknet](darknet.md) or [CNTK](cntk.md) and compile them
for Raspberry Pi.

*Important:* Before proceeding, first run the demo python script for the model of your choice (cntkDemo.py or darknetDemo.py). Besides loading the models, each script will convert the neural net to a .map file that represents the model in ELL's format. Depending on the neural network, the file can get quite large.

*Important:* Make sure you have already built ELL in the `Release` configuration, so that you have the ELL compilation tools ready.

When you build ELL you will get a copy of the tutorial/vision/gettingStarted folder
copied to your `build` folder with some additional build generated scripts.

This page explains how the `compileDarknetModel.cmd` (or `compileDarknetModel.sh`) works.
This generated script compiles the darknetReference.map file generated by darknetDemo.py so you can run it on your PC.

*Linux instructions are similar except with forward slashes for path separators
and invoking gcc or clang instead of the MSVC `cl` and `link` command.*

## Windows 64 bit

First, we need to establish where your ELL root directory is, if you are in the build/tutorials/vision/gettingStarted folder then this should be the root:

    set ELL_ROOT=..\..\..\..
    
Next we need to be able to find the ELL compiler which lives here:

    set PATH=%PATH%;%ELL_ROOT%\build\bin\Release

Now run this to compile the darknet model to LLVM intermediate representation (IR).

    compile -imap darknetReference.map -cfn darknetReference -of darknetReference.ll

*Note:* this may take a minute or more.

We need to also generate the python wrapper code for the model:

    compile -imap darknetReference.map -cfn darknetReference -cmn darknetReference -o swig -of darknetReference.i

*Note:* this may also take a minute or more.

Next we need to run SWIG to generate the C++ python wrappers.  If you installed SWIG manually then it is probably
already in your PATH, if not then you will find it here (on Windows):

    set PATH=%PATH%;%ELL_ROOT%\external\swigwintools.3.0.12\tools\swigwin-3.0.12
    
Then run:

    swig -python -modern -c++ -Fmicrosoft -py3 -outdir . -c++ -I%ELL_ROOT%/interfaces/common/include -I%ELL_ROOT%/interfaces/common -I%ELL_ROOT%/libraries/emitters/include -o darknetPYTHON_wrap.cxx darknetReference.i

This should be quick.  Next we run `llc` to compile the IR language generated by compile step above
into a .obj linkable module.  This means we need to be able to find llc:

    set PATH=%PATH%;%ELL_ROOT%\external\LLVMLibs.3.9.0.1\build\native\tools\

Then run this:

    llc -filetype=obj darknet.ll -march x86-64

*Note:* thie machine architecture we chose there means you need to be running it on 64 bit Windows.
`llc` supports many other targets, including what we need to run on Raspberry Pi.

Now we can build the python module using the MSVC compiler.  This means you need to open the
Visual Studio 2015 X64 Native Tools Command Prompt, to ensure you are building 64 bit executable.

The compiler and linker will need to be able to find your Python 3.6 SDK.  If you type `where python`
you will see where the Python executable lives, then set the following variable to point to that location:

    set PY_ROOT=d:\Continuum\Anaconda2\envs\py36

In my case I was using Anaconda.  Now you can run the Visual Studio C++ compiler to compile the Python wrapper:

    cl /I%PY_ROOT%\include /I%ELL_ROOT%\interfaces\common\include\ /I%ELL_ROOT%\libraries\emitters\include\ /c /EHsc /MD darknetPYTHON_wrap.cxx
Â 
And we finish up with the Visual Studio C++ linker to produce the Python loadable module.
The linker may also need to find the LibOpenBlas library which lives.   OpenBlas is a library that is optimized
for specific types of CPUs, right down to the CPU model number.  So you need to pick the right processor type.
For Intell, we support either haswell or sandybridge.  Note that Ivy Bridge is compatible with Sandy Bridge, and Broadwell is compatible with Haswell.  When you ran `cmake` for ELL part of the cmake output will tell you what processor family you have, you should see some output like this:

    -- Processor family: 6, model: 79
    -- Using OpenBLAS compiled for haswell

So this means I need to use this version of OpenBlas:
 
    set OPENBLAS=%ELL_ROOT%\external\OpenBLASLibs.0.2.19.2\build\native\x64\haswell\lib

Then run the linker:

    link darknetPYTHON_wrap.obj darknet.obj %PY_ROOT%\libs\python35.lib %OPENBLAS%\libopenblas.dll.a /MACHINE:x64 /SUBSYSTEM:WINDOWS /DLL /DEBUG:FULL /PDB:_darknet.pdb /OUT:_darknet.pyd

## Run the Compiled Model

Ok, now that _darknet.pyd exists, we can load it up into Python and see if it works. From your
Anaconda Python 3.6 environment run this:

    Python compiledDarknetDemo.py

If this fails to load, it could be because it is failing to find the OpenBlas library, so do this:

    set PATH=%PATH%;%OPENBLAS%

(using the OPENBLAS variable we defined earlier on this page) then try again.

One thing you should notice is that this compiled version of darknet loads instantly and runs quickly.
